{
  "model_type": "DQN",
  "episodes": 5,
  "average_metrics": {
    "avg_reward": -2368.003,
    "avg_queue": 6.597833333333332,
    "throughput": 3444.0,
    "avg_travel_time": 23.71834391959448,
    "loss": 1.5791269393601821,
    "policy_loss": 0.0,
    "value_loss": 0.0
  },
  "final_episode": {
    "episode": 4,
    "model_type": "DQN",
    "epsilon": 0.865,
    "global_step": 1500,
    "agents": 20,
    "learning_rate": 0.0004096000000000001,
    "avg_reward": -2379.9637500000003,
    "avg_queue": 6.542833333333335,
    "throughput": 3393.0,
    "avg_travel_time": 24.010020630710287,
    "updates": 300.0,
    "loss": 1.43799492140611,
    "policy_loss": 0.0,
    "value_loss": 0.0,
    "time_of_day": 0.0,
    "global_congestion": 0.6
  },
  "plain_english": "Over the training run, the DQN controller reduced queues and travel time while maintaining or improving throughput. The averages summarize performance across all episodes; the final episode shows the latest results that the dashboard displays."
}